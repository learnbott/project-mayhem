{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded...\n",
      "context_window=128000 num_output=256 is_chat_model=True is_function_calling_model=True model_name='qwen2.5:3b-instruct-q8_0' system_role=<MessageRole.SYSTEM: 'system'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: notify if ollama server is running with model loaded\n",
    "import subprocess, os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "model_name, ctx_len = \"qwen2.5:3b-instruct-q8_0\", 128000\n",
    "# ollama pull hf.co/mradermacher/SaulLM-54B-Instruct-i1-GGUF:Q6_K\n",
    "# ollama pull hf.co/mradermacher/SaulLM-54B-Instruct-i1-GGUF:Q4_K_M\n",
    "# ollama pull hf.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF:Q4_K_M\n",
    "\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=8000)\n",
    "else:\n",
    "    subout = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    if model_name in subout.stdout:\n",
    "        print('Model loaded...')\n",
    "    else:\n",
    "        try: \n",
    "            print(\"Pulling Ollama model...\")\n",
    "            sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "        except Exception as e: \n",
    "            print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    system_prompt = \"You are training an new Portfolio Manager of a hedgefund.\"\n",
    "    additional_kwargs = {\"num_predict\": 4000}\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                 request_timeout=4000.0, additional_kwargs=additional_kwargs, json_mode=False) #, system_prompt=system_prompt)\n",
    "    print(llm.metadata)\n",
    "\n",
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pdf: https://assets.openstax.org/oscms-prodcms/media/documents/PrinciplesofFinance-WEB.pdf\n",
      "Started parsing the file under job_id bca0ff4f-ad37-4a77-99a7-988ca9351bb7\n",
      "............"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import Document\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/project-mayhem/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llama_api_key = os.getenv(\"LLAMA_API_KEY\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={}, save_json_path=None):\n",
    "    \n",
    "    parser = LlamaParse(api_key=llama_api_key, **llamaparse_kwargs)\n",
    "    \n",
    "    documents = []\n",
    "    for pdf in pdf_urls:\n",
    "        print('processing pdf:', pdf)\n",
    "        documents += parser.load_data(pdf)\n",
    "\n",
    "    if save_json_path:\n",
    "        with open(save_json_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "            documents.append(Document(text=result['text']))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "pages_to_extract = \"\"\n",
    "beginning_of_chapter = 21\n",
    "end_of_chapter = 24 # 644\n",
    "for i in range(beginning_of_chapter,end_of_chapter):\n",
    "    if i == end_of_chapter - 1:\n",
    "        pages_to_extract += str(i)\n",
    "    else:\n",
    "        pages_to_extract += str(i) + \",\"\n",
    "principles_of_finance = \"https://assets.openstax.org/oscms-prodcms/media/documents/PrinciplesofFinance-WEB.pdf\"\n",
    "documents = extract_text_from_pdf([principles_of_finance], llama_api_key, llamaparse_kwargs={\"split_by_page\": False, \"target_pages\": pages_to_extract}, save_json_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc57df3c484fd489c2b2a9646011df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcabe729171e4a40ab86980b57ad07b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 5\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Embedding model\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start \n",
    "embed_model_name = \"bge-m3\"\n",
    "embed_model = OllamaEmbedding(embed_model_name, base_url=\"http://localhost:11435\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, embed_model=embed_model, include_metadata=True)\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "data_path = \"microsoft/orca-agentinstruct-1M-v1\"\n",
    "dataset = load_dataset(data_path)\n",
    "orca_keys = list(dataset.keys())\n",
    "\n",
    "okey = orca_keys[1]\n",
    "teststr = dataset[okey][0]['messages']\n",
    "# Convert the string to a list of dictionaries\n",
    "list_of_dicts = json.loads(teststr)\n",
    "\n",
    "print(okey)\n",
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruction_prompts import all_questions\n",
    "from parse_instruction_output import all_parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = list(all_questions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(all_questions['word_definition'].format(financial_text=nodes[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "def generate_random_hash():\n",
    "    random_uuid = uuid.uuid4()\n",
    "    hash_object = hashlib.sha256(random_uuid.bytes)\n",
    "    return hash_object.hexdigest()\n",
    "import os, json\n",
    "from instruction_prompts import all_questions\n",
    "from parse_instruction_output import all_parsers\n",
    "\n",
    "if not os.path.exists('/workspace/data/uvu_lit/pm_synth_train_dataset'):\n",
    "    os.makedirs('/workspace/data/uvu_lit/pm_synth_train_dataset')\n",
    "\n",
    "# for task in all_questions:\n",
    "############################################################\n",
    "task='question_answering_generation_from_facts'\n",
    "############################################################\n",
    "print(f\"  Processing {task}...\")\n",
    "corpus = {}\n",
    "collection = {}\n",
    "output_path = f\"/workspace/data/uvu_lit/pm_synth_train_dataset/dataset_{task}.json\"\n",
    "corpus_path = f\"/workspace/data/uvu_lit/pm_synth_train_dataset/corpus_{task}.json\"\n",
    "for node_counter, node in enumerate(nodes[:3]):\n",
    "    instruct_prompt = all_questions[task]        \n",
    "    parser = all_parsers[task]\n",
    "    prompt = instruct_prompt.format(financial_text=node.text)\n",
    "    \n",
    "    while True:\n",
    "        response = llm.complete(prompt)\n",
    "        clean_response = parser(response.text)\n",
    "        \n",
    "        counter = 0\n",
    "        if clean_response == []:\n",
    "            if counter == 2:\n",
    "                print(f\"    Failed to parse {task} for {node.metadata['filename']}...\")\n",
    "                break\n",
    "            counter += 1\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(clean_response, str):\n",
    "                clean_response = [clean_response]\n",
    "\n",
    "            hashes = [generate_random_hash() for _ in range(len(clean_response))]\n",
    "            \n",
    "            corpus[node.node_id] = node.text\n",
    "            for i in range(len(hashes)):\n",
    "                collection[hashes[i]] = {'response': clean_response[i],\n",
    "                                        'hash_id': hashes[i],\n",
    "                                        'relevant_doc': node.node_id,\n",
    "                                        'task': task}\n",
    "            break\n",
    "    \n",
    "    if node_counter % max(5,int(len(nodes) / 10)) == 0 or node_counter == len(nodes) - 1:\n",
    "        if os.path.exists(output_path):\n",
    "            with open(output_path, 'a') as f:\n",
    "                json.dump(collection, f)\n",
    "            collection = {}\n",
    "        else:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(collection, f)\n",
    "            \n",
    "        if os.path.exists(corpus_path):\n",
    "            with open(corpus_path, 'a') as f:\n",
    "                json.dump(corpus, f)\n",
    "            corpus = {}\n",
    "        else:\n",
    "            with open(corpus_path, 'w') as f:\n",
    "                json.dump(corpus, f)\n",
    "        print(f\"    Processed {node_counter+1}/{len(nodes)} nodes...\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
