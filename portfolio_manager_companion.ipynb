{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded...\n",
      "context_window=128000 num_output=256 is_chat_model=True is_function_calling_model=True model_name='qwen2.5:3b-instruct-q8_0' system_role=<MessageRole.SYSTEM: 'system'>\n"
     ]
    }
   ],
   "source": [
    "# TODO: notify if ollama server is running with model loaded\n",
    "import subprocess, os\n",
    "from llama_index.core import Document\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI as LOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/agentic-ai/.env')\n",
    "\n",
    "# model_name, ctx_len = \"llama3.1:8b-instruct-q8_0\", 128000\n",
    "model_name, ctx_len = \"qwen2.5:3b-instruct-q8_0\", 128000\n",
    "# ollama pull hf.co/mradermacher/SaulLM-54B-Instruct-i1-GGUF:Q6_K\n",
    "# ollama pull hf.co/mradermacher/SaulLM-54B-Instruct-i1-GGUF:Q4_K_M\n",
    "# ollama pull hf.co/bartowski/Llama-3.1-Nemotron-70B-Instruct-HF-GGUF:Q4_K_M\n",
    "\n",
    "\n",
    "if \"gpt-4o\" in model_name:\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    print(f\"Using OpenAI {model_name}...\")\n",
    "    llm = LOpenAI(model=model_name, max_tokens=8000)\n",
    "else:\n",
    "    subout = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n",
    "    if model_name in subout.stdout:\n",
    "        print('Model loaded...')\n",
    "    else:\n",
    "        try: \n",
    "            print(\"Pulling Ollama model...\")\n",
    "            sub_out = subprocess.run(['ollama', 'pull', model_name], capture_output=True, text=True)\n",
    "        except Exception as e: \n",
    "            print(f\"Error pulling model: Is the Ollama server running?\\n{e}\")\n",
    "    \n",
    "    system_prompt = \"You are training an new Portfolio Manager of a hedgefund.\"\n",
    "    additional_kwargs = {\"num_predict\": 4000}\n",
    "    llm = Ollama(model=model_name, url=\"http://127.0.0.1:11434\", context_window=ctx_len, model_type=\"chat\", is_function_calling_model=True, \n",
    "                 request_timeout=4000.0, additional_kwargs=additional_kwargs, json_mode=False) #, system_prompt=system_prompt)\n",
    "    print(llm.metadata)\n",
    "\n",
    "# Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pdf: https://assets.openstax.org/oscms-prodcms/media/documents/PrinciplesofFinance-WEB.pdf\n",
      "Started parsing the file under job_id 651fa41d-e39f-4097-950c-a09636cee606\n",
      "............"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import Document\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/workspace/repos/project-mayhem/.env')\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llama_api_key = os.getenv(\"LLAMA_API_KEY\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_urls, llama_api_key, llamaparse_kwargs={}, save_json_path=None):\n",
    "    \n",
    "    parser = LlamaParse(api_key=llama_api_key, **llamaparse_kwargs)\n",
    "    \n",
    "    documents = []\n",
    "    for pdf in pdf_urls:\n",
    "        print('processing pdf:', pdf)\n",
    "        documents += parser.load_data(pdf)\n",
    "\n",
    "    if save_json_path:\n",
    "        with open(save_json_path, \"r\") as f:\n",
    "            result = json.load(f)\n",
    "            documents.append(Document(text=result['text']))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "pages_to_extract = \"\"\n",
    "beginning_of_chapter = 21\n",
    "end_of_chapter = 54 # 644\n",
    "for i in range(beginning_of_chapter,end_of_chapter):\n",
    "    if i == end_of_chapter - 1:\n",
    "        pages_to_extract += str(i)\n",
    "    else:\n",
    "        pages_to_extract += str(i) + \",\"\n",
    "principles_of_finance = \"https://assets.openstax.org/oscms-prodcms/media/documents/PrinciplesofFinance-WEB.pdf\"\n",
    "documents = extract_text_from_pdf([principles_of_finance], llama_api_key, llamaparse_kwargs={\"split_by_page\": False, \"target_pages\": pages_to_extract}, save_json_path=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mSemanticSplitterNodeParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_metadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minclude_prev_next_rel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCallbackManager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mid_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFieldInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeforeValidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0m_validate_id_func\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f6290bfd260\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_schema_input_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPydanticUndefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serialization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPlainSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0m_serialize_id_func\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f6290bfda80\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPydanticUndefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'always'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msentence_splitter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serialization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membed_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnnotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSerializeAsAny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbreakpoint_percentile_threshold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Semantic node parser.\n",
      "\n",
      "Splits a document into Nodes, with each node being a group of semantically related sentences.\n",
      "\n",
      "Args:\n",
      "    buffer_size (int): number of sentences to group together when evaluating semantic similarity\n",
      "    embed_model: (BaseEmbedding): embedding model to use\n",
      "    sentence_splitter (Optional[Callable]): splits text into sentences\n",
      "    include_metadata (bool): whether to include metadata in nodes\n",
      "    include_prev_next_rel (bool): whether to include prev/next relationships\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "validated to form a valid model.\n",
      "\n",
      "`self` is explicitly positional-only to allow `self` as a field name.\n",
      "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/llama_index/core/node_parser/text/semantic_splitter.py\n",
      "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "SemanticSplitterNodeParser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/workspace/data/principles_of_finance.json\", \"w\") as f:\n",
    "    json.dump([d.text for d in documents], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/workspace/data/principles_of_finance.json\", \"r\") as f:\n",
    "    documents = [Document(text=text) for text in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# Embedding model\n",
    "# OLLAMA_HOST=\"http://127.0.0.1:11435\" ollama start \n",
    "embed_model_name = \"bge-m3\"\n",
    "embed_model = OllamaEmbedding(embed_model_name, base_url=\"http://localhost:11435\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, embed_model=embed_model, include_metadata=True)\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "data_path = \"microsoft/orca-agentinstruct-1M-v1\"\n",
    "dataset = load_dataset(data_path)\n",
    "orca_keys = list(dataset.keys())\n",
    "\n",
    "okey = orca_keys[1]\n",
    "teststr = dataset[okey][0]['messages']\n",
    "# Convert the string to a list of dictionaries\n",
    "list_of_dicts = json.loads(teststr)\n",
    "\n",
    "print(okey)\n",
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruction_prompts import all_questions\n",
    "from parse_instruction_output import all_parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = list(all_questions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(all_questions['word_definition'].format(financial_text=nodes[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "def generate_random_hash():\n",
    "    random_uuid = uuid.uuid4()\n",
    "    hash_object = hashlib.sha256(random_uuid.bytes)\n",
    "    return hash_object.hexdigest()\n",
    "import os, json\n",
    "from instruction_prompts import all_questions\n",
    "from parse_instruction_output import all_parsers\n",
    "\n",
    "if not os.path.exists('/workspace/data/uvu_lit/pm_synth_train_dataset'):\n",
    "    os.makedirs('/workspace/data/uvu_lit/pm_synth_train_dataset')\n",
    "\n",
    "# for task in all_questions:\n",
    "############################################################\n",
    "task='word_definition'\n",
    "############################################################\n",
    "print(f\"  Processing {task}...\")\n",
    "corpus = {}\n",
    "collection = {}\n",
    "output_path = f\"/workspace/data/uvu_lit/pm_synth_train_dataset/dataset_{task}.json\"\n",
    "corpus_path = f\"/workspace/data/uvu_lit/pm_synth_train_dataset/corpus_{task}.json\"\n",
    "for node_counter, node in enumerate(nodes):\n",
    "    instruct_prompt = all_questions[task]        \n",
    "    parser = all_parsers[task]\n",
    "    prompt = instruct_prompt.format(financial_text=node.text, num_count=2)\n",
    "    \n",
    "    while True:\n",
    "        response = llm.complete(prompt)\n",
    "        clean_response = parser(response.text)\n",
    "        \n",
    "        counter = 0\n",
    "        if clean_response == []:\n",
    "            if counter == 2:\n",
    "                print(f\"    Failed to parse {task} for {node.metadata['filename']}...\")\n",
    "                break\n",
    "            counter += 1\n",
    "            continue\n",
    "        else:\n",
    "            if isinstance(clean_response, str):\n",
    "                clean_response = [clean_response]\n",
    "\n",
    "            hashes = [generate_random_hash() for _ in range(len(clean_response))]\n",
    "            \n",
    "            corpus[node.node_id] = node.text\n",
    "            for i in range(len(hashes)):\n",
    "                collection[hashes[i]] = {'response': clean_response[i],\n",
    "                                        'hash_id': hashes[i],\n",
    "                                        'relevant_doc': node.node_id,\n",
    "                                        'task': task}\n",
    "            break\n",
    "    \n",
    "    if node_counter % max(3,int(len(nodes) / 10)) == 0 or node_counter == len(nodes) - 1:\n",
    "        if os.path.exists(output_path):\n",
    "            with open(output_path, 'a') as f:\n",
    "                json.dump(collection, f)\n",
    "            collection = {}\n",
    "        else:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(collection, f)\n",
    "            \n",
    "        if os.path.exists(corpus_path):\n",
    "            with open(corpus_path, 'a') as f:\n",
    "                json.dump(corpus, f)\n",
    "            corpus = {}\n",
    "        else:\n",
    "            with open(corpus_path, 'w') as f:\n",
    "                json.dump(corpus, f)\n",
    "        print(f\"    Processed {node_counter+1}/{len(nodes)} nodes...\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
